{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    source_int = [[source_vocab_to_int[word] for word in line.split()] for line in source_text.split('\\n')]\n",
    "    target_int = [[target_vocab_to_int[word] for word in line.split()] + [target_vocab_to_int['<EOS>']] for line in target_text.split('\\n')]\n",
    "    \n",
    "    return source_int, target_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32,(None,None),name='input')\n",
    "    targets = tf.placeholder(tf.int32,(None,None),name='targets')\n",
    "    lr = tf.placeholder(tf.float32,name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    target_sequence_length = tf.placeholder(tf.int32,(None,),name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)\n",
    "    source_sequence_length = tf.placeholder(tf.int32,(None,),name='source_sequence_length')\n",
    "    return inputs, targets, lr, keep_prob, target_sequence_length, max_target_len, source_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    processed_data = tf.strided_slice(target_data,[0,0],[batch_size,-1],[1,1])\n",
    "    processed_data = tf.concat([tf.fill([batch_size,1],target_vocab_to_int['<GO>']),processed_data],axis=1)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imp import reload\n",
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :param source_sequence_length: a list of the lengths of each sequence in the batch\n",
    "    :param source_vocab_size: vocabulary size of source data\n",
    "    :param encoding_embedding_size: embedding size of source data\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(rnn_inputs, vocab_size = source_vocab_size, embed_dim=encoding_embedding_size)\n",
    "    def make_cell(rnn_size, keep_prob):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "        drop_cell = tf.contrib.rnn.DropoutWrapper(enc_cell, output_keep_prob=keep_prob)\n",
    "        return drop_cell\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    # Helper for training process. Used by BasicDecoder to read input\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,sequence_length=target_sequence_length,time_major=False)\n",
    "        \n",
    "    # Basic decoder\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer=output_layer)\n",
    "        \n",
    "    # Perform dynamic decoding using the decoder\n",
    "    training_decoder_output, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, impute_finished=True, maximum_iterations=max_summary_length)\n",
    "    return training_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding - Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):    \n",
    "    start_tokens = tf.tile([start_of_sequence_id], [batch_size], name='start_tokens')\n",
    "    # Helper for inference process\n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, start_tokens=start_tokens, end_token=end_of_sequence_id)\n",
    "    # Basic decoder\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, encoder_state, output_layer=output_layer)\n",
    "    # Perform dynamic decoding using the decoder\n",
    "    inference_decoder_output,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, impute_finished=True, maximum_iterations=max_target_sequence_length)\n",
    "    return inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \n",
    "    # Decode embedding\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    # Construct the decode cell\n",
    "    def make_cell(rnn_size,keep_prob):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "        drop_cell = tf.contrib.rnn.DropoutWrapper(dec_cell,output_keep_prob=keep_prob)\n",
    "        return drop_cell\n",
    "    \n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    # Dense layer to translate the decoder's output into a choice from target vocabulary\n",
    "    output_layer = Dense(target_vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    # Get the training and inference logits\n",
    "    with tf.variable_scope(\"decoder\") as scope:\n",
    "        training_decoder_output  = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n",
    "        scope.reuse_variables()\n",
    "        inference_decoder_output = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, target_vocab_size, output_layer, batch_size, keep_prob)\n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \n",
    "    enc_output, enc_state =  encoding_layer(input_data, rnn_size, num_layers, keep_prob, source_sequence_length, source_vocab_size, enc_embedding_size)\n",
    "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(dec_input, enc_state, target_sequence_length, max_target_sentence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)\n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 256\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 300\n",
    "decoding_embedding_size = 300\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length = model_inputs()\n",
    "\n",
    "    #sequence_length = tf.placeholder_with_default(max_target_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   10/538 - Train Accuracy: 0.2611, Validation Accuracy: 0.3501, Loss: 3.9831\n",
      "Epoch   0 Batch   20/538 - Train Accuracy: 0.3214, Validation Accuracy: 0.3672, Loss: 3.3111\n",
      "Epoch   0 Batch   30/538 - Train Accuracy: 0.3207, Validation Accuracy: 0.3952, Loss: 3.2068\n",
      "Epoch   0 Batch   40/538 - Train Accuracy: 0.3777, Validation Accuracy: 0.3748, Loss: 2.7412\n",
      "Epoch   0 Batch   50/538 - Train Accuracy: 0.2904, Validation Accuracy: 0.3622, Loss: 2.7862\n",
      "Epoch   0 Batch   60/538 - Train Accuracy: 0.2846, Validation Accuracy: 0.3576, Loss: 2.6040\n",
      "Epoch   0 Batch   70/538 - Train Accuracy: 0.2950, Validation Accuracy: 0.3443, Loss: 2.2554\n",
      "Epoch   0 Batch   80/538 - Train Accuracy: 0.2469, Validation Accuracy: 0.3143, Loss: 2.1610\n",
      "Epoch   0 Batch   90/538 - Train Accuracy: 0.2894, Validation Accuracy: 0.3255, Loss: 1.9314\n",
      "Epoch   0 Batch  100/538 - Train Accuracy: 0.3080, Validation Accuracy: 0.3839, Loss: 1.8219\n",
      "Epoch   0 Batch  110/538 - Train Accuracy: 0.3365, Validation Accuracy: 0.4190, Loss: 1.7840\n",
      "Epoch   0 Batch  120/538 - Train Accuracy: 0.3203, Validation Accuracy: 0.4070, Loss: 1.6605\n",
      "Epoch   0 Batch  130/538 - Train Accuracy: 0.3674, Validation Accuracy: 0.4231, Loss: 1.4899\n",
      "Epoch   0 Batch  140/538 - Train Accuracy: 0.3252, Validation Accuracy: 0.3874, Loss: 1.5214\n",
      "Epoch   0 Batch  150/538 - Train Accuracy: 0.3609, Validation Accuracy: 0.4036, Loss: 1.3567\n",
      "Epoch   0 Batch  160/538 - Train Accuracy: 0.3724, Validation Accuracy: 0.4158, Loss: 1.2376\n",
      "Epoch   0 Batch  170/538 - Train Accuracy: 0.3893, Validation Accuracy: 0.4055, Loss: 1.1874\n",
      "Epoch   0 Batch  180/538 - Train Accuracy: 0.4018, Validation Accuracy: 0.4226, Loss: 1.1369\n",
      "Epoch   0 Batch  190/538 - Train Accuracy: 0.3819, Validation Accuracy: 0.4462, Loss: 1.1500\n",
      "Epoch   0 Batch  200/538 - Train Accuracy: 0.3943, Validation Accuracy: 0.4492, Loss: 1.0719\n",
      "Epoch   0 Batch  210/538 - Train Accuracy: 0.4154, Validation Accuracy: 0.4522, Loss: 1.0287\n",
      "Epoch   0 Batch  220/538 - Train Accuracy: 0.4087, Validation Accuracy: 0.4478, Loss: 0.9905\n",
      "Epoch   0 Batch  230/538 - Train Accuracy: 0.3732, Validation Accuracy: 0.4545, Loss: 1.0019\n",
      "Epoch   0 Batch  240/538 - Train Accuracy: 0.3963, Validation Accuracy: 0.4616, Loss: 1.0199\n",
      "Epoch   0 Batch  250/538 - Train Accuracy: 0.3672, Validation Accuracy: 0.4590, Loss: 0.9454\n",
      "Epoch   0 Batch  260/538 - Train Accuracy: 0.3943, Validation Accuracy: 0.4466, Loss: 0.9476\n",
      "Epoch   0 Batch  270/538 - Train Accuracy: 0.3795, Validation Accuracy: 0.4469, Loss: 0.9509\n",
      "Epoch   0 Batch  280/538 - Train Accuracy: 0.4297, Validation Accuracy: 0.4446, Loss: 0.8727\n",
      "Epoch   0 Batch  290/538 - Train Accuracy: 0.3992, Validation Accuracy: 0.4705, Loss: 0.8994\n",
      "Epoch   0 Batch  300/538 - Train Accuracy: 0.4180, Validation Accuracy: 0.4677, Loss: 0.8752\n",
      "Epoch   0 Batch  310/538 - Train Accuracy: 0.3684, Validation Accuracy: 0.4268, Loss: 0.8817\n",
      "Epoch   0 Batch  320/538 - Train Accuracy: 0.4191, Validation Accuracy: 0.4732, Loss: 0.8916\n",
      "Epoch   0 Batch  330/538 - Train Accuracy: 0.4364, Validation Accuracy: 0.4657, Loss: 0.8411\n",
      "Epoch   0 Batch  340/538 - Train Accuracy: 0.3715, Validation Accuracy: 0.4846, Loss: 0.9091\n",
      "Epoch   0 Batch  350/538 - Train Accuracy: 0.4230, Validation Accuracy: 0.4686, Loss: 0.8699\n",
      "Epoch   0 Batch  360/538 - Train Accuracy: 0.4184, Validation Accuracy: 0.4830, Loss: 0.8719\n",
      "Epoch   0 Batch  370/538 - Train Accuracy: 0.4213, Validation Accuracy: 0.4957, Loss: 0.8760\n",
      "Epoch   0 Batch  380/538 - Train Accuracy: 0.4398, Validation Accuracy: 0.4862, Loss: 0.8267\n",
      "Epoch   0 Batch  390/538 - Train Accuracy: 0.4637, Validation Accuracy: 0.4961, Loss: 0.7984\n",
      "Epoch   0 Batch  400/538 - Train Accuracy: 0.4660, Validation Accuracy: 0.4901, Loss: 0.8050\n",
      "Epoch   0 Batch  410/538 - Train Accuracy: 0.4475, Validation Accuracy: 0.4993, Loss: 0.8199\n",
      "Epoch   0 Batch  420/538 - Train Accuracy: 0.4467, Validation Accuracy: 0.4908, Loss: 0.7953\n",
      "Epoch   0 Batch  430/538 - Train Accuracy: 0.4508, Validation Accuracy: 0.5069, Loss: 0.7956\n",
      "Epoch   0 Batch  440/538 - Train Accuracy: 0.4600, Validation Accuracy: 0.4931, Loss: 0.8262\n",
      "Epoch   0 Batch  450/538 - Train Accuracy: 0.4944, Validation Accuracy: 0.5014, Loss: 0.7953\n",
      "Epoch   0 Batch  460/538 - Train Accuracy: 0.4663, Validation Accuracy: 0.5036, Loss: 0.7506\n",
      "Epoch   0 Batch  470/538 - Train Accuracy: 0.4851, Validation Accuracy: 0.5197, Loss: 0.7654\n",
      "Epoch   0 Batch  480/538 - Train Accuracy: 0.5061, Validation Accuracy: 0.5135, Loss: 0.7463\n",
      "Epoch   0 Batch  490/538 - Train Accuracy: 0.4905, Validation Accuracy: 0.5272, Loss: 0.7615\n",
      "Epoch   0 Batch  500/538 - Train Accuracy: 0.5156, Validation Accuracy: 0.5119, Loss: 0.6995\n",
      "Epoch   0 Batch  510/538 - Train Accuracy: 0.4286, Validation Accuracy: 0.4506, Loss: 0.7988\n",
      "Epoch   0 Batch  520/538 - Train Accuracy: 0.4475, Validation Accuracy: 0.5050, Loss: 0.7963\n",
      "Epoch   0 Batch  530/538 - Train Accuracy: 0.4309, Validation Accuracy: 0.4931, Loss: 0.7988\n",
      "Epoch   1 Batch   10/538 - Train Accuracy: 0.4330, Validation Accuracy: 0.5121, Loss: 0.7861\n",
      "Epoch   1 Batch   20/538 - Train Accuracy: 0.4781, Validation Accuracy: 0.5078, Loss: 0.7608\n",
      "Epoch   1 Batch   30/538 - Train Accuracy: 0.4871, Validation Accuracy: 0.5117, Loss: 0.7834\n",
      "Epoch   1 Batch   40/538 - Train Accuracy: 0.5481, Validation Accuracy: 0.5142, Loss: 0.6583\n",
      "Epoch   1 Batch   50/538 - Train Accuracy: 0.4824, Validation Accuracy: 0.5400, Loss: 0.7532\n",
      "Epoch   1 Batch   60/538 - Train Accuracy: 0.4893, Validation Accuracy: 0.5087, Loss: 0.7357\n",
      "Epoch   1 Batch   70/538 - Train Accuracy: 0.5043, Validation Accuracy: 0.5389, Loss: 0.7040\n",
      "Epoch   1 Batch   80/538 - Train Accuracy: 0.4900, Validation Accuracy: 0.5344, Loss: 0.7548\n",
      "Epoch   1 Batch   90/538 - Train Accuracy: 0.5365, Validation Accuracy: 0.5458, Loss: 0.7231\n",
      "Epoch   1 Batch  100/538 - Train Accuracy: 0.5148, Validation Accuracy: 0.5380, Loss: 0.7114\n",
      "Epoch   1 Batch  110/538 - Train Accuracy: 0.5150, Validation Accuracy: 0.5613, Loss: 0.7284\n",
      "Epoch   1 Batch  120/538 - Train Accuracy: 0.5275, Validation Accuracy: 0.5426, Loss: 0.6926\n",
      "Epoch   1 Batch  130/538 - Train Accuracy: 0.5247, Validation Accuracy: 0.5611, Loss: 0.6862\n",
      "Epoch   1 Batch  140/538 - Train Accuracy: 0.4938, Validation Accuracy: 0.5676, Loss: 0.7513\n",
      "Epoch   1 Batch  150/538 - Train Accuracy: 0.5441, Validation Accuracy: 0.5609, Loss: 0.7087\n",
      "Epoch   1 Batch  160/538 - Train Accuracy: 0.5173, Validation Accuracy: 0.5563, Loss: 0.6720\n",
      "Epoch   1 Batch  170/538 - Train Accuracy: 0.5298, Validation Accuracy: 0.5540, Loss: 0.6905\n",
      "Epoch   1 Batch  180/538 - Train Accuracy: 0.5605, Validation Accuracy: 0.5513, Loss: 0.6800\n",
      "Epoch   1 Batch  190/538 - Train Accuracy: 0.5320, Validation Accuracy: 0.5428, Loss: 0.6902\n",
      "Epoch   1 Batch  200/538 - Train Accuracy: 0.5586, Validation Accuracy: 0.5662, Loss: 0.6823\n",
      "Epoch   1 Batch  210/538 - Train Accuracy: 0.5472, Validation Accuracy: 0.5634, Loss: 0.6781\n",
      "Epoch   1 Batch  220/538 - Train Accuracy: 0.5266, Validation Accuracy: 0.5680, Loss: 0.6574\n",
      "Epoch   1 Batch  230/538 - Train Accuracy: 0.5279, Validation Accuracy: 0.5497, Loss: 0.7009\n",
      "Epoch   1 Batch  240/538 - Train Accuracy: 0.5451, Validation Accuracy: 0.5700, Loss: 0.6828\n",
      "Epoch   1 Batch  250/538 - Train Accuracy: 0.5266, Validation Accuracy: 0.5515, Loss: 0.6651\n",
      "Epoch   1 Batch  260/538 - Train Accuracy: 0.5272, Validation Accuracy: 0.5763, Loss: 0.6745\n",
      "Epoch   1 Batch  270/538 - Train Accuracy: 0.5176, Validation Accuracy: 0.5630, Loss: 0.6827\n",
      "Epoch   1 Batch  280/538 - Train Accuracy: 0.5789, Validation Accuracy: 0.5705, Loss: 0.6471\n",
      "Epoch   1 Batch  290/538 - Train Accuracy: 0.5170, Validation Accuracy: 0.5652, Loss: 0.6661\n",
      "Epoch   1 Batch  300/538 - Train Accuracy: 0.5545, Validation Accuracy: 0.5563, Loss: 0.6408\n",
      "Epoch   1 Batch  310/538 - Train Accuracy: 0.5270, Validation Accuracy: 0.5534, Loss: 0.6845\n",
      "Epoch   1 Batch  320/538 - Train Accuracy: 0.5545, Validation Accuracy: 0.5740, Loss: 0.6468\n",
      "Epoch   1 Batch  330/538 - Train Accuracy: 0.5499, Validation Accuracy: 0.5744, Loss: 0.6357\n",
      "Epoch   1 Batch  340/538 - Train Accuracy: 0.5064, Validation Accuracy: 0.5691, Loss: 0.7045\n",
      "Epoch   1 Batch  350/538 - Train Accuracy: 0.5565, Validation Accuracy: 0.5691, Loss: 0.6487\n",
      "Epoch   1 Batch  360/538 - Train Accuracy: 0.5500, Validation Accuracy: 0.5696, Loss: 0.6645\n",
      "Epoch   1 Batch  370/538 - Train Accuracy: 0.5441, Validation Accuracy: 0.5645, Loss: 0.6689\n",
      "Epoch   1 Batch  380/538 - Train Accuracy: 0.5398, Validation Accuracy: 0.5664, Loss: 0.6375\n",
      "Epoch   1 Batch  390/538 - Train Accuracy: 0.5843, Validation Accuracy: 0.5598, Loss: 0.6278\n",
      "Epoch   1 Batch  400/538 - Train Accuracy: 0.5616, Validation Accuracy: 0.5668, Loss: 0.6362\n",
      "Epoch   1 Batch  410/538 - Train Accuracy: 0.5420, Validation Accuracy: 0.5708, Loss: 0.6591\n",
      "Epoch   1 Batch  420/538 - Train Accuracy: 0.5768, Validation Accuracy: 0.5732, Loss: 0.6499\n",
      "Epoch   1 Batch  430/538 - Train Accuracy: 0.5510, Validation Accuracy: 0.5749, Loss: 0.6502\n",
      "Epoch   1 Batch  440/538 - Train Accuracy: 0.5432, Validation Accuracy: 0.5653, Loss: 0.6608\n",
      "Epoch   1 Batch  450/538 - Train Accuracy: 0.5614, Validation Accuracy: 0.5632, Loss: 0.6359\n",
      "Epoch   1 Batch  460/538 - Train Accuracy: 0.5532, Validation Accuracy: 0.5914, Loss: 0.6064\n",
      "Epoch   1 Batch  470/538 - Train Accuracy: 0.5744, Validation Accuracy: 0.5749, Loss: 0.6178\n",
      "Epoch   1 Batch  480/538 - Train Accuracy: 0.5755, Validation Accuracy: 0.5696, Loss: 0.6203\n",
      "Epoch   1 Batch  490/538 - Train Accuracy: 0.5718, Validation Accuracy: 0.5854, Loss: 0.6296\n",
      "Epoch   1 Batch  500/538 - Train Accuracy: 0.5875, Validation Accuracy: 0.5866, Loss: 0.5787\n",
      "Epoch   1 Batch  510/538 - Train Accuracy: 0.5876, Validation Accuracy: 0.5909, Loss: 0.6201\n",
      "Epoch   1 Batch  520/538 - Train Accuracy: 0.5719, Validation Accuracy: 0.5861, Loss: 0.6276\n",
      "Epoch   1 Batch  530/538 - Train Accuracy: 0.5650, Validation Accuracy: 0.5936, Loss: 0.6294\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     source_sequence_length: sources_lengths,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     source_sequence_length: valid_sources_lengths,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'problem_unittests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-411c269a2d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mproblem_unittests\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource_vocab_to_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource_int_to_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_int_to_vocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'problem_unittests'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence to Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    sentence_lower = sentence.lower()\n",
    "    sentence_int = [vocab_to_int[word] if word in vocab_to_int else vocab_to_int['<UNK>'] for word in sentence_lower.split()]\n",
    "    return sentence_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         source_sequence_length: [len(translate_sentence)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
